{"cells":[{"metadata":{"_uuid":"fe23a579df98fdeb1c7dc3d75f072288d7f9934a","_execution_state":"idle","_cell_guid":"63c5232d-817a-4fa9-aab1-aacfe3c919d1","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Submission Funtion**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Importing Dataset \n**\n\nImporting and and reading the training and testing data with the help of pandas packages"},{"metadata":{"_uuid":"c95e83e9a460f6728c7bd70d88674f481cbf55d5","_execution_state":"idle","_cell_guid":"46da69c5-7a30-42a6-bcfa-bbfe416803c8","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/train.csv\")\ntest_data = pd.read_csv(\"../input/test.csv\")\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Data Cleaning and Preparation **\n\n**   2.1  Identifying and handling missing data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data = train_data.isnull()\nfor column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu','GarageQual','GarageCond','GarageFinish','GarageType', 'Functional', 'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2', 'Utilities']\n\n# replace 'NaN' with 'None' in these columns\nfor col in cols_fillna:\n    train_data[col].fillna('None',inplace=True)\n    test_data[col].fillna('None',inplace=True)\n\n# replacing with mode\nmode_col = ['MSZoning','Electrical', 'KitchenQual','Exterior2nd', 'Exterior1st','SaleType']\nfor col in mode_col:\n    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n    test_data[col] = test_data[col].fillna(test_data[col].mode()[0])\n\n# replcaing with zero \nfor col in (\"GarageYrBlt\",\"MasVnrArea\"): \n    train_data[col] = train_data[col].fillna(0)\n\ntest_zero_Columns= ['GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\"MasVnrArea\"]\nfor col in test_zero_Columns:\n    test_data[col] = test_data[col].fillna(0)\n    \n#replacing with median\ntrain_data[\"LotFrontage\"].replace(np.nan,train_data[\"LotFrontage\"].median(axis=0),inplace = True)\ntest_data[\"LotFrontage\"].replace(np.nan,train_data[\"LotFrontage\"].median(axis=0),inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"   **2.2 Dealing with Ouliers**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"array = [ 'GrLivArea']\nfor x in array : \n    fig, ax = plt.subplots()\n    ax.scatter(train_data[x], train_data['SalePrice'])\n    plt.ylabel('SalePrice', fontsize=13)\n    plt.xlabel(x, fontsize=13)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop(train_data[train_data['LotFrontage']>300].index)\ntrain_data = train_data.drop(train_data[(train_data['LotFrontage']>140) & (train_data['SalePrice']<100000)].index)\ntrain_data = train_data.drop(train_data[train_data['LotArea']>40000].index)\ntrain_data = train_data.drop(train_data[train_data['BsmtFinSF1']>5000].index)\ntrain_data = train_data.drop(train_data[(train_data['GrLivArea']>4000) & (train_data['SalePrice']<300000)].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arr = ['LotArea']\nfor x in arr : \n    train_data[x].plot(kind = 'box')  \n    plt.xlabel(x)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_outlier(df,array):\n    low = .05\n    high = .95\n    quant_df = df.quantile([low, high])\n    for name in array:\n        Q1 = df[name].quantile(0.25)\n        Q3 = df[name].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_limit = float(Q1 - 1.5 * IQR)\n        upper_limit = float(Q3 + 2 * IQR)\n        df = df[(df[name] > lower_limit) \n               & (df[name] < upper_limit)]\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"array = ['LotArea','BsmtFinSF1','YearBuilt']\nprint(train_data.shape)\ntrain_data = remove_outlier(train_data,array)\nprint(train_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n   ** 2 . 3   Skewness and Log Transformation **"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_feats = train_data.dtypes[train_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = train_data[numeric_feats].apply(lambda x: x.skew())\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\nskewness = skewness.dropna(axis = 0)\nskewness = skewness.drop('SalePrice')\nprint(skewness)\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    train_data[feat] = boxcox1p(train_data[feat], lam)\n    test_data[feat] = boxcox1p(test_data[feat], lam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.4 Processing Categorical Variables**\n    \nDealing with ordinal data- the following are the ordinal variables in the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"fireplaceQc_ord_map = {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex': 5}\ntrain_data['FireplaceQu'] = train_data['FireplaceQu'].map(fireplaceQc_ord_map)\ntest_data['FireplaceQu'] = test_data['FireplaceQu'].map(fireplaceQc_ord_map)\n\npoolQc_ord_map = {'None':0,'Fa':2,'Gd':4,'Ex': 5}\ntrain_data['PoolQC'] = train_data['PoolQC'].map(poolQc_ord_map)\ntest_data['PoolQC'] = test_data['PoolQC'].map(poolQc_ord_map)\n\nExterQual_ord_map = {'Fa':2,'TA':3,'Gd':4,'Ex': 5}\ntrain_data['ExterQual'] = train_data['ExterQual'].map(ExterQual_ord_map)\ntest_data['ExterQual'] = test_data['ExterQual'].map(ExterQual_ord_map)\n\nExterCond_ord_map = {'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex': 5}\ntrain_data['ExterCond'] = train_data['ExterCond'].map(ExterCond_ord_map)\ntest_data['ExterCond'] = test_data['ExterCond'].map(ExterCond_ord_map)\n\nBsmtQual_ord_map = {'None':0,'Fa':2,'TA':3,'Gd':4,'Ex': 5}\ntrain_data['BsmtQual'] = train_data['BsmtQual'].map(BsmtQual_ord_map)\ntest_data['BsmtQual'] = test_data['BsmtQual'].map(BsmtQual_ord_map)\n\nBsmtCond_ord_map = {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4}\ntrain_data['BsmtCond'] = train_data['BsmtCond'].map(BsmtCond_ord_map)\ntest_data['BsmtCond'] = test_data['BsmtCond'].map(BsmtCond_ord_map)\n\nHeatingQC_ord_map = {'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex': 5}\ntrain_data['HeatingQC'] = train_data['HeatingQC'].map(HeatingQC_ord_map)\ntest_data['HeatingQC'] = test_data['HeatingQC'].map(HeatingQC_ord_map)\n\nKitchenQual_ord_map = {'None':0,'Fa':2,'TA':3,'Gd':4,'Ex': 5}\ntrain_data['KitchenQual'] = train_data['KitchenQual'].map(KitchenQual_ord_map)\ntest_data['KitchenQual'] = test_data['KitchenQual'].map(KitchenQual_ord_map)\n\nGarageQual_ord_map = {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex': 5}\ntrain_data['GarageQual'] = train_data['GarageQual'].map(GarageQual_ord_map)\ntest_data['GarageQual'] = test_data['GarageQual'].map(GarageQual_ord_map)\n\n\nGarageCond_ord_map = {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex': 5}\ntrain_data['GarageCond'] = train_data['GarageCond'].map(GarageCond_ord_map)\ntest_data['GarageCond'] = test_data['GarageCond'].map(GarageCond_ord_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Factorize method**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_fact = train_data.copy()\ntest_data_fact = test_data.copy()\ncolnames1 = train_data.select_dtypes(include='object').columns\nfor col in colnames1:\n    train_data_fact[col] = pd.factorize(train_data[col])[0]\ncolnames2 = test_data.select_dtypes(include='object').columns\nfor col in colnames2:\n    test_data_fact[col] = pd.factorize(test_data[col])[0] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dummy method**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_dum = train_data.copy()\ntest_data_dum = test_data.copy()\ncoln= ['MSZoning','Electrical','Functional','SaleType','Condition2',\"HouseStyle\",\"RoofMatl\",\"Exterior1st\",\"Exterior2nd\",\"Heating\",\"CentralAir\",\"KitchenQual\"]\nfor col in coln:\n    train_data_dum[col] = pd.factorize(train_data_dum[col])[0] \nfor col in coln:\n    test_data_dum[col] = pd.factorize(test_data_dum[col])[0]\n\n#dummy_array = [\"LotShape\",\"LandContour\",\"LotConfig\",\"LandSlope\",\"Neighborhood\",\"Condition1\",\"BldgType\",\"RoofStyle\",\"MasVnrType\",\"Foundation\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"GarageType\",\"GarageFinish\",\"PavedDrive\",\"SaleCondition\"]\ntrain_data_dum = pd.get_dummies(train_data_dum)\ntest_data_dum = pd.get_dummies(test_data_dum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_fact.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Feature Engineering **\n\n**3.1 Creating new Variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_fact[\"TotalArea\"] = test_data_fact[\"TotalBsmtSF\"]+test_data_fact[\"1stFlrSF\"]+test_data_fact[\"2ndFlrSF\"]\ntrain_data_fact[\"TotalArea\"] = train_data_fact[\"TotalBsmtSF\"]+train_data_fact[\"1stFlrSF\"]+train_data_fact[\"2ndFlrSF\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_fact[\"BathRooms\"] = test_data_fact[\"FullBath\"]+0.5*test_data_fact[\"HalfBath\"]\ntrain_data_fact[\"BathRooms\"] = train_data_fact[\"FullBath\"]+0.5*train_data_fact[\"HalfBath\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_fact[\"BsmtBath\"] = train_data_fact[\"BsmtFullBath\"]+0.5*train_data_fact[\"BsmtHalfBath\"]\ntest_data_fact[\"BsmtBath\"] = test_data_fact[\"BsmtFullBath\"]+0.5*test_data_fact[\"BsmtHalfBath\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_fact[\"Year_avg\"] = (train_data_fact[\"YearRemodAdd\"]+train_data_fact[\"YearBuilt\"])/2\ntest_data_fact[\"Year_avg\"] = (test_data_fact[\"YearRemodAdd\"]+test_data_fact[\"YearBuilt\"])/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_fact[\"BsmtSF\"] = train_data_fact[\"BsmtFinSF1\"]+train_data_fact[\"BsmtFinSF2\"]\ntest_data_fact[\"BsmtSF\"] = test_data_fact[\"BsmtFinSF1\"]+test_data_fact[\"BsmtFinSF2\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train_data_fact[[\"BsmtSF\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"SalePrice\"]].corr()\ncorr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.2 Statistical Analysis**\n\nBivariate Analysis : Statistical Tests can be used to select those features that has strongest relation with the target feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nX = train_data_fact.iloc[:,0:80]\nY = train_data_fact[[\"SalePrice\"]]\n\n#apply SelectKBest class to extract top 10 best features\n    \nbestfeatures = SelectKBest(score_func=chi2, k=15)\nfit = bestfeatures.fit(X,Y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(15,'Score'))  #print 10 best features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.3 Pearson's Correlation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get correlations of each features in dataset\ncorrmat = train_data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(train_data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_quant = train_data_fact.select_dtypes(exclude=['object'])\ncorr = train_quant.corr()\nprint(corr[\"SalePrice\"].sort_values(ascending = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.4  Greedy Algorithm**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\nlm = LinearRegression()\n\ndef LinearRegressionModelWithSplit(X,Y,index):\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n    model = lm.fit(X_train,y_train)\n    score = lm.score(X_train,y_train)\n    Y_pred = model.predict(X_train)\n    Y_pred[Y_pred < 0] = 0\n    #cross_val_score(lm, X_train, y_train, scoring=\"neg_mean_squared_log_error\", cv = 5)\n    rmsle_train = np.sqrt(-cross_val_score(lm, X_train, y_train, scoring=\"neg_mean_squared_log_error\", cv = 5))\n    #rmsle_train = np.sqrt(mean_squared_log_error(y_train, Y_pred))      \n    Y_pred_test = model.predict(X_test)\n    Y_pred_test[Y_pred_test < 0] = 0\n    \n    rmsle_test = np.sqrt(mean_squared_log_error(y_test, Y_pred_test))  \n    array = np.array([index,X.columns,score,rmsle_train.mean(),rmsle_test])\n    return array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data = train_data_fact\n#X = pd.DataFrame(my_imputer.fit_transform(X))\nX = X_data.drop([\"SalePrice\",'Id'],axis = 1)\ndf_col = X.columns\ncolumns = [\"Feature\",\"Columns\",\"Score\",\"RMSE_Train\",\"RMSE_test\"]\n\ndef quatitativeFeatureSelection():\n    m = 0\n    array = []\n    index = []\n    df_final = pd.DataFrame()\n    x = pd.DataFrame()\n    while m < 30:         \n        df = pd.DataFrame()    \n        for i in range(72):\n            x= X.iloc[:,i:i+1] \n            if(len(index) != 0):\n                for val in index:\n                    k = int(val)\n                    name = X.iloc[:,k:k+1].columns                    \n                    x[name] = X.iloc[:,k:k+1]\n            #print(x.columns)    \n            y = np.log(X_data[[\"SalePrice\"]]+1)\n            array = pd.Series(LinearRegressionModelWithSplit(x,y,i))\n            df = df.append(array,ignore_index=True)\n            \n        df.columns = columns\n        min_cross= df['RMSE_Train'].min(axis = 0)\n        df_final = df_final.append(df[df['RMSE_Train']== min_cross])\n        index.append(df[df['RMSE_Train']== min_cross][\"Feature\"].values[0])        \n        m=m+1\n    return df_final\nGreedy_df = quatitativeFeatureSelection()\nGreedy_col = Greedy_df.iloc[:,1:2]\nGreedy_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = Greedy_df.iloc[15,1:2]\nfor i in col:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Model Development **\n\n **4.1 Multiple Linear Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib\n\ndef ResidualPlot(X,Y,model):    \n    matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n    Y_pred = model.predict(X)\n    preds = pd.DataFrame(Y)\n    preds['pred'] = Y_pred  \n    preds[\"residuals\"] = preds[\"SalePrice\"] - preds[\"pred\"]\n    preds.plot(x = \"pred\", y = \"residuals\",kind = \"scatter\")\\\n    \ndef logsubmission(X_test, model,ID):        \n    Y_pred = np.expm1(model.predict(X_test))\n    \n    Y_df = pd.DataFrame(ID)\n    Y_df['SalePrice'] = Y_pred    \n    Y_df.to_csv('test.csv',index=False)\n    return Y_df\n\ndef RegressionModelDevelopment(feature_array,train,test):\n    X = train[feature_array]\n    Y = np.log(train[\"SalePrice\"]+1)   \n    print(LinearRegressionModelWithSplit(X,Y,2))\n    model = LinearRegression().fit(X,Y)\n    ResidualPlot(X,Y,model)\n    df = pd.DataFrame(logsubmission(test[feature_array],model,test['Id']))\n    #link = create_download_link(df) \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Best Features obtrained from greedy algorithm\n#features = [\"TotalBsmtSF\",'BsmtExposure', 'OverallQual', 'GrLivArea', 'YearBuilt', 'BsmtFinSF1','OverallCond', 'LotArea', 'GarageCars', 'FireplaceQu','BldgType', 'KitchenQual', 'Condition1', 'Functional', 'PavedDrive','HeatingQC', 'ScreenPorch', 'LotFrontage', 'CentralAir', 'YearRemodAdd','Street', 'GarageArea', 'BsmtFullBath', 'FullBath', 'HalfBath','LandContour', 'ExterQual', 'KitchenAbvGr', 'PoolQC']\nfeatures = ['BedroomAbvGr', 'OverallQual', 'GrLivArea', 'BsmtFinSF1', 'YearBuilt','OverallCond', 'LotArea', 'TotalBsmtSF', 'KitchenQual', 'GarageCars','Functional', 'HeatingQC', 'FireplaceQu', 'ScreenPorch', 'BldgType','ExterQual', 'BsmtFullBath', 'Condition1', 'WoodDeckSF', 'KitchenAbvGr','FullBath', 'GarageArea', 'YearRemodAdd', 'LowQualFinSF', 'Condition2']      \ndf = RegressionModelDevelopment(features,train_data_fact,test_data_fact)\ncreate_download_link(df) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **4.2 Ridge Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\n\ndef bestAlpha(X,Y,model):\n    parameters= {'alpha':[x for x in range(1,101)]}\n\n    ridge_reg=GridSearchCV(model, param_grid=parameters, scoring='neg_mean_squared_log_error', cv=15)\n    ridge_reg.fit(X,Y)\n    print(\"The best value of Alpha is: \",ridge_reg.best_params_)\n    print(\"The best score achieved with Alpha=20 is: \",np.sqrt(-ridge_reg.best_score_))\n    return ridge_reg.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RidgeModelDevelopment(feature_array,train,test):\n    X = train[feature_array]\n    Y = np.log(train[\"SalePrice\"]+1)       \n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n    \n    alpha = bestAlpha(X,Y,Ridge())    \n    ridge_mod=Ridge(alpha=alpha,normalize = True)\n    ridge_mod.fit(X_train,y_train)\n    y_pred_train=ridge_mod.predict(X_train)\n    cross_val_score(ridge_mod, X_train, y_train, scoring=\"neg_mean_squared_log_error\", cv = 5)\n    rmsle_train = np.sqrt(-cross_val_score(ridge_mod, X_train, y_train, scoring=\"neg_mean_squared_log_error\", cv = 5))\n    print(\"Root mean squared log error train:\"+str(rmsle_train.mean()))\n    print('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(y_train, y_pred_train)))) \n    \n    y_pred_test=ridge_mod.predict(X_test)        \n    rmsle_test = np.sqrt(mean_squared_log_error(y_test, y_pred_test)) \n    print(\"Root mean squared log error test:\"+str(rmsle_test))\n    print('Root Mean Square error test = ' + str(np.sqrt(mean_squared_error(y_test, y_pred_test)))) \n    \n    print(ridge_mod.coef_)\n    #model = LinearRegression().fit(X,Y)\n    ResidualPlot(X,Y,ridge_mod)\n    df = pd.DataFrame(logsubmission(test[feature_array],ridge_mod,test['Id']))\n    link = create_download_link(df) \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = RidgeModelDevelopment(features,train_data_fact,test_data_fact)\ncreate_download_link(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Model Ridge regression\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\nX_train = train_data_fact[['BsmtFullBath', 'OverallQual', 'GrLivArea', 'YearBuilt', 'BsmtFinSF1','OverallCond', 'TotalArea', 'LotArea', 'GarageCars', 'KitchenQual','Functional', 'FireplaceQu', 'HeatingQC', 'ScreenPorch', 'BldgType','ExterQual']]\n\ny = np.log(train_data_fact[[\"SalePrice\"]]+1)\n\ncross_val_score(Ridge(), X_train, y, scoring=\"neg_mean_squared_log_error\", cv = 5)\nrmse_ridge = np.sqrt(-cross_val_score(Ridge(), X_train, y, scoring=\"neg_mean_squared_log_error\", cv = 5))\nrmse_ridge\n\nalphas = [1e-15,1e-10,1e-8,1e-4,1e-3,0.01,0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_log_error\", cv = 5))\n    return(rmse)\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **4.3 Lasso Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso = Lasso()\ndef LassoModelDevelopment(feature_array,train,test):\n    X = train[feature_array]\n    Y = np.log(train[\"SalePrice\"]+1)       \n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n    \n    alpha = bestAlpha(X,Y,Lasso())    \n    lasso_mod=Lasso(alpha=alpha,normalize = True)\n    lasso_mod.fit(X_train,y_train)\n    y_pred_train=lasso_mod.predict(X_train)\n    cross_val_score(lasso_mod, X_train, y_train, scoring=\"neg_mean_squared_log_error\", cv = 5)\n    rmsle_train = np.sqrt(-cross_val_score(lasso_mod, X_train, y_train, scoring=\"neg_mean_squared_log_error\", cv = 5))\n    print(\"Root mean squared log error train:\"+str(rmsle_train.mean()))\n    print('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(y_train, y_pred_train)))) \n    \n    y_pred_test=lasso_mod.predict(X_test)        \n    rmsle_test = np.sqrt(mean_squared_log_error(y_test, y_pred_test)) \n    print(\"Root mean squared log error test:\"+str(rmsle_test))\n    print('Root Mean Square error test = ' + str(np.sqrt(mean_squared_error(y_test, y_pred_test)))) \n    \n    print(lasso_mod.coef_)\n    #model = LinearRegression().fit(X,Y)\n    ResidualPlot(X,Y,lasso_mod)\n    df = pd.DataFrame(logsubmission(test[feature_array],lasso_mod,test['Id']))\n    link = create_download_link(df) \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LassoModelDevelopment(features,train_data_fact,test_data_fact)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4.4 Ensemble Modeling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingRegressor\nfrom sklearn import tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['BedroomAbvGr', 'OverallQual', 'GrLivArea', 'BsmtFinSF1', 'YearBuilt','OverallCond', 'LotArea', 'TotalBsmtSF', 'KitchenQual', 'GarageCars','Functional', 'HeatingQC', 'FireplaceQu', 'ScreenPorch', 'BldgType','ExterQual', 'BsmtFullBath', 'Condition1', 'WoodDeckSF', 'KitchenAbvGr','FullBath', 'GarageArea', 'YearRemodAdd', 'LowQualFinSF', 'Condition2']\nX = train_data_fact[features]\nY = np.log(train_data_fact[\"SalePrice\"]+1)\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\nensemble_col = ['Model_Name','Score','Train_rmsle','Test_rmsle']\nensemble_df = pd.DataFrame(columns =ensemble_col )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef Model(model,ensemble_df):\n    reg = model.fit(x_train,y_train)\n    score = model.score(x_train,y_train)\n    y_pred =  model.predict(x_test)\n    rmsle_train = np.sqrt(-cross_val_score(model,x_train,y_train,scoring = \"neg_mean_squared_log_error\",cv = 5))\n    rmsle_test = np.sqrt(mean_squared_log_error(y_test, y_pred)) \n    ensemble_df = ensemble_df.append({'Model_Name' : str(model) , 'Score' : score,'Train_rmsle':rmsle_train.mean(),\"Test_rmsle\":rmsle_test},ignore_index=True)\n    print(\"Score \"+str(score))\n    print(\"rmsle_train \"+str(rmsle_train.mean()))\n    print(\"rmsle_test \"+str(rmsle_test))\n    print(\" \")\n    return ensemble_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Regression Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = LinearRegression()\nensemble_df = Model(lm,ensemble_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid = GridSearchCV(lm,parameters, cv=None)\nensemble_df = Model(lm,ensemble_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Ridge Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = make_pipeline(RobustScaler(), Ridge(alpha =10, random_state=1))\nensemble_df = Model(ridge,ensemble_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Lasso Regression **"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.001, random_state=1))\nensemble_df = Model(lasso,ensemble_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4.  Bagging Meta- estimator**"},{"metadata":{"trusted":true},"cell_type":"code","source":"bagging = BaggingRegressor(tree.DecisionTreeRegressor(random_state=1))\nensemble_df = Model(bagging,ensemble_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5.  GradientBoosting**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nGBR= GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,max_depth=4, max_features='sqrt',min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =5)\nensemble_df = Model(GBR,ensemble_df)\ndf = pd.DataFrame(logsubmission(test_data_fact[features],GBR,test_data_fact['Id']))\n#create_download_link(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6. XGBoost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nxgb_reg=xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3,min_child_weight=1.7817, n_estimators=2200,reg_alpha=0.4640, reg_lambda=0.8571,subsample=0.5213, silent=1,random_state =7, nthread = -1) \nensemble_df = Model(xgb_reg,ensemble_df)                             \n                    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_reg1=xgb.XGBRegressor(base_score=0.5, booster='gbtree',colsample_bylevel=1, colsample_bynode=1,colsample_bytree=1, gamma=0,importance_type='gain', learning_rate=0.1,max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,n_estimators=100, n_jobs=1, nthread=-1,objective='reg:linear', random_state=0,reg_alpha=0, reg_lambda=1,scale_pos_weight=1, seed=None, silent=None,subsample=1, verbosity=1)\nensemble_df = Model(xgb_reg1,ensemble_df)     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**7. Voting Regressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nridge_mod = Ridge(alpha = 20)\nlasso_mod = Lasso(alpha = 1)\nvote_mod = VotingRegressor([('Ridge', ridge_mod), ('lm',LinearRegression())])\nensemble_df = Model(vote_mod,ensemble_df)\ndf = pd.DataFrame(logsubmission(test_data_fact[features],vote_mod,test_data_fact['Id']))\n#create_download_link(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**8. Stacking Regressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.regressor import StackingRegressor\n\nlm = LinearRegression()\nstregr = StackingRegressor(regressors=[ridge_mod,lm, vote_mod,lasso_mod], meta_regressor=xgb_reg, use_features_in_secondary=True)\nensemble_df = Model(stregr,ensemble_df)\n#df = pd.DataFrame(logsubmission(test_data_fact[features],stack_mod,test_data_fact['Id']))\n#create_download_link(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**9. Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor \n# Create a random forest classifier\nclf = RandomForestRegressor(n_estimators=1000, random_state=0, n_jobs=-1)\nX = train_data_dum\nY = train_data_dum['SalePrice']\nX = train_data_dum.drop(\"SalePrice\",axis =1)\n\n# Train the classifier\nclf.fit(X,Y)\nnames = X.columns\nprint (\"Features sorted by their score:\")\nRF_feat = pd.DataFrame(sorted(zip(map(lambda x: round(x, 4), clf.feature_importances_), names),reverse=True))\n             ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # create regressor object \nregressor = RandomForestRegressor(n_estimators = 400, random_state = 0) \n  \n# fit the regressor with x and y data \nensemble_df = Model(regressor,ensemble_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**9. Average Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_data_fact.values)\n    rmse= np.sqrt(-cross_val_score(model, x_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"averaged_models = AveragingModels(models = (stregr, GBR, ridge))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = averaged_models.fit(x_train,y_train)\ndf = pd.DataFrame(logsubmission(test_data_fact[features],model1,test_data_fact['Id']))\ncreate_download_link(df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"file_extension":".py","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":1}